# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = regex_filter
a1.sources.r1.interceptors.i1.regex = ^[a-z_]+
# 设置为false, 只有匹配上正则的事件才会通过
a1.sources.r1.interceptors.i1.excludeEvents = false

# Describe the sink(logger是直接打印到控制台)
a1.sinks.k1.type=logger
# Maximum number of bytes of the Event body to log, 默认值是16, 如果使用默认值, 当event比较长时, 会被截断
maxBytesToLog=1024

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 启动agent代理:
# $flume-ng agent -n a1 -f source-interceptor-timestamp-conf.properties
# file-exec.log中的内容:
#test exec source
#test flume
#test flume 2
#hahahaha
#test file channel
#Test hdfs sink
#file roll sink
# 日志输出(Test hdfs sink这条数据被过滤掉了):
#17/08/23 09:32:25 INFO sink.LoggerSink: Event: { headers:{} body: 74 65 73 74 20 65 78 65 63 20 73 6F 75 72 63 65 test exec source }
#17/08/23 09:32:25 INFO sink.LoggerSink: Event: { headers:{} body: 74 65 73 74 20 66 6C 75 6D 65                   test flume }
#17/08/23 09:32:25 INFO sink.LoggerSink: Event: { headers:{} body: 74 65 73 74 20 66 6C 75 6D 65 20 32             test flume 2 }
#17/08/23 09:32:25 INFO sink.LoggerSink: Event: { headers:{} body: 68 61 68 61 68 61 68 61                         hahahaha }
#17/08/23 09:32:25 INFO sink.LoggerSink: Event: { headers:{} body: 74 65 73 74 20 66 69 6C 65 20 63 68 61 6E 6E 65 test file channe }
#17/08/23 09:32:25 INFO sink.LoggerSink: Event: { headers:{} body: 66 69 6C 65 20 72 6F 6C 6C 20 73 69 6E 6B       file roll sink }
##########################################################