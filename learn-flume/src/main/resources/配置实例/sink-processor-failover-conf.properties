# Describe the source
a1.sources=r1
a1.sinks=k1 k2
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log

a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2
a1.sinkgroups.g1.processor.type = failover
a1.sinkgroups.g1.processor.priority.k1 = 5
a1.sinkgroups.g1.processor.priority.k2 = 10

# Describe the sink
a1.sinks.k1.type=hdfs
# /flume/event/目录需要先创建, hadoop fs -mkdir -p /flume/event/
a1.sinks.k1.hdfs.path=/flume/event/%y%m%d
a1.sinks.k1.hdfs.filePrefix=sink-processor-failover-
#是否轮转生成新的hdfs文件
a1.sinks.k1.hdfs.round=true
#每一分钟轮转一次, 生成新的文件, 但是没有接收到event的话是不会每分钟都生成新的文件的
a1.sinks.k1.hdfs.roundValue=1
a1.sinks.k1.hdfs.roundUnit=minute
# 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp=true
a1.sinks.k1.hdfs.writeFormat=Text

# Describe the sink
a1.sinks.k2.type=hbase
a1.sinks.k2.table=flume_test02
a1.sinks.k2.columnFamily=cf1
a1.sinks.k2.serializer=org.apache.flume.sink.hbase.RegexHbaseEventSerializer

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1
#a1.sinks.k2.channel=c1

#################################################
# 在Hbase中创建表flume_test02, sh hbase shell
# create 'flume_test02', 'cf1'
# 启动agent代理:
# $flume-ng agent -n a1 -f sink-processor-failover-conf.properties
# 实际我们配置的时候, 没有加a1.sinks.k2.channel=c1, 报错了: org.apache.flume.conf.ConfigurationException: No channel configured for sink: k2, 然后只创建了k1, 然后数据就写到了hdfs中
# 会先在目录里面写.tmp临时文件, 写好后改成不带.tmp的文件, 比如: /flume/event/170821/sink-processor-failover-.1503289473688.tmp, 最终变成: /flume/event/170821/sink-processor-failover-.1503289473688
# 使用hadoop fs -text /flume/event/170821/sink-processor-failover-.1503289473688查看文件内容:
#1503289475034   test exec source
#1503289475037   test flume
#1503289475038   test flume 2
#1503289475039   hahahaha
#1503289475040   test file channel
#1503289475041   Test hdfs sink
#1503289475042   file roll sink
# 由于我们在配置中已经指定为text, 所以这里显示的是文本内容
# 而进入hbase, 通过命令: scan 'flume_test02'
# 发现hbase表中是没有任何内容的, 这是因为k2出现了异常, k1是正常的, 所以使用k1来处理event
# 如果我们加上配置a1.sinks.k2.channel=c1再启动, 则数据被写到了hbase中:
#hbase(main):004:0> scan 'flume_test02'
#ROW                                              COLUMN+CELL
#1503295952648-EEj9niN487-0                      column=cf1:payload, timestamp=1503295955824, value=test exec source
#1503295952652-EEj9niN487-1                      column=cf1:payload, timestamp=1503295955824, value=test flume
#1503295952652-EEj9niN487-2                      column=cf1:payload, timestamp=1503295955824, value=test flume 2
#1503295952653-EEj9niN487-3                      column=cf1:payload, timestamp=1503295955824, value=hahahaha
#1503295952653-EEj9niN487-4                      column=cf1:payload, timestamp=1503295955824, value=test file channel
#1503295952653-EEj9niN487-5                      column=cf1:payload, timestamp=1503295955824, value=Test hdfs sink
#1503295952653-EEj9niN487-6                      column=cf1:payload, timestamp=1503295955824, value=file roll sink
#7 row(s) in 0.0930 seconds
##########################################################