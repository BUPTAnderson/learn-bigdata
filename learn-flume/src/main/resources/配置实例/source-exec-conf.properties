# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log

# Describe the sink(logger是直接打印到控制台)
a1.sinks.k1.type=logger
# Maximum number of bytes of the Event body to log, 默认值是16, 如果使用默认值, 当event比较长时, 会被截断
maxBytesToLog=1024

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 启动agent代理:
# $flume-ng agent -n a1 -f source-exec-conf.properties
# 在另一个session中准备源数据
# $echo "test exec source" > file-exec.log
# 这时另一个窗口中已经可以看到数据了
# 继续追加数据(是>> 如果是>是overwrite是接收不到的)
# echo "test flume" >> file-exec.log
# 通过ctrl + c将服务停掉, 模拟宕机情况
# 可以继续向file-exec.log追加内容, 不追加也可以
# 再把exec source启动起来
# 发现, 每次重新启动source-exec-conf.properties, 通过tail -F每次都会默认从文件的最后10条来进行采集, 所以每次重新启动服务的话, 有可能造成数据重复采集(重启的这段时间内没有数据产生或者产生的数据少于10条)
# 有可能造成数据丢失(重启服务的这段时间产生的新数据大于10条)
# 应对策略:
# 如果是分区数据重启服务的话, 首先把当天的数据删除, 重新采集今天的数据, 这时候通过 grep -rn '2017-12-12 00:00:00' file-exec.log找到今天的数据所在的行数number
# 把配置文件中的tail -F /home/hadoop/data/file-exec.log, 改为tail -F -n +number /home/hadoop/data/file-exec.log, 这样就从number行开始采集数据了.
##########################################################