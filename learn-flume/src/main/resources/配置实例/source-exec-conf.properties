# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log

# Describe the sink(logger是直接打印到控制台)
a1.sinks.k1.type=logger
# Maximum number of bytes of the Event body to log, 默认值是16, 如果使用默认值, 当event比较长时, 会被截断
maxBytesToLog=1024

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 启动agent代理:
# $flume-ng agent -n a1 -f source-exec-conf.properties
# 在另一个session中准备源数据
# $echo "test exec source" > file-exec.log
# 这时另一个窗口中已经可以看到数据了
# 继续追加数据(是>> 如果是>是overwrite是接收不到的)
# echo "test flume" >> file-exec.log
# 通过ctrl + c将服务停掉, 模拟宕机情况
# 可以继续向file-exec.log追加内容, 不追加也可以
# 再把exec source启动起来
# 发现, exec source把file-exec.log又从头读取了一遍, 这也就是exec source的问题, 服务异常停止后再启动会重新读取文件的内容, 会造成数据重复
##########################################################