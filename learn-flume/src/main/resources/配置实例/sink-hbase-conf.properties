# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log

# Describe the sink
a1.sinks.k1.type=hbase
a1.sinks.k1.table=flume_test01
a1.sinks.k1.columnFamily=cf1
a1.sinks.k1.serializer=org.apache.flume.sink.hbase.RegexHbaseEventSerializer

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 启动hbase
# ./hbase shell
# 创建表(末尾不要带分号)
# hbase(main):002:0> create 'flume_test01' , 'cf1'
# 通过list可以看到表创建成功
# 启动agent代理:
# ./flume-ng agent -n a1 -f ../conf/sink-hbase-conf.properties
# 在hbas通过如下命令查看是否有数据进入:
# scan 'flume_test01'
# 结果如下:
#ROW                                              COLUMN+CELL
#1503285058665-oGF9bR1UYX-0                      column=cf1:payload, timestamp=1503285061851, value=test exec source
#1503285058669-oGF9bR1UYX-1                      column=cf1:payload, timestamp=1503285061851, value=test flume
#1503285058669-oGF9bR1UYX-2                      column=cf1:payload, timestamp=1503285061851, value=test flume 2
#1503285058669-oGF9bR1UYX-3                      column=cf1:payload, timestamp=1503285061851, value=hahahaha
#1503285058670-oGF9bR1UYX-4                      column=cf1:payload, timestamp=1503285061851, value=test file channel
#1503285058670-oGF9bR1UYX-5                      column=cf1:payload, timestamp=1503285061851, value=Test hdfs sink
#1503285058670-oGF9bR1UYX-6                      column=cf1:payload, timestamp=1503285061851, value=file roll sink
#7 row(s) in 0.2180 seconds
##########################################################