# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=TAILDIR
# 存放文件偏移量的json文件
a1.sources.r1.positionFile = /tmp/flume/taildir_position.json
a1.sources.r1.filegroups = f1 f2
# filegroups为f1对应的目录文件
a1.sources.r1.filegroups.f1 = /home/hadoop/data/test1/example.log
# filegroups为f1的在hearder中增加key:headerKey1, value:test1
a1.sources.r1.headers.f1.headerKey1 = test1
# filegroups为f2对应的目录文件格式
a1.sources.r1.filegroups.f2 = /home/hadoop/data/test2/.*log.*
# filegroups为f2的在hearder中增加key:headerKey1, value:test2
a1.sources.r1.headers.f2.headerKey1 = test2
# filegroups为f2的在hearder中增加key:headerKey2, value:value2-2
a1.sources.r1.headers.f2.headerKey2 = value2-2
# 增加key来标识file的绝对路径, key的值默认是file, 也可通过fileHeaderKey自定义
a1.sources.r1.fileHeader = true
#a1.sources.r1.fileHeaderKey = file

# Describe the sink(logger是直接打印到控制台)
a1.sinks.k1.type=logger
a1.sinks.k1.maxBytesToLog=1024

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 启动agent代理(在004启动服务):
# $flume-ng agent -n a1 -f source-taildir-conf.properties
# 终端输出:
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test1, file=/home/hadoop/data/test1/example.log} body: 68 65 6C 6C 6F 20 77 6F 72 6C 64                hello world }
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test1, file=/home/hadoop/data/test1/example.log} body: 65 78 61 6D 70 6C 65                            example }
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test1, file=/home/hadoop/data/test1/example.log} body: 74 65 73 74 20 66 6C 75 6D 65                   test flume }
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test2, headerKey2=value2-2, file=/home/hadoop/data/test2/catalina.2017-08-18.log} body: 32 30 31 37 2D 30 38 2D 31 38                   2017-08-18 }
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test2, headerKey2=value2-2, file=/home/hadoop/data/test2/catalina.2017-08-28.log} body: 74 65 73 74 20 74 61 69 6C 20 64 69 72          test tail dir }
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test2, headerKey2=value2-2, file=/home/hadoop/data/test2/catalina.2017-08-28.log} body: 61 70 61 63 68 65 20 66 6C 75 6D 65             apache flume }
# 17/08/28 15:23:45 INFO sink.LoggerSink: Event: { headers:{headerKey1=test2, headerKey2=value2-2, file=/home/hadoop/data/test2/catalina.2017-08-28.log} body: 32 30 31 37 2D 30 38 2D 32 38                   2017-08-28 }
#17/08/28 15:25:50 INFO taildir.TaildirSource: Closed file: /home/hadoop/data/test1/example.log, inode: 1201034, pos: 31
#17/08/28 15:25:50 INFO taildir.TaildirSource: Closed file: /home/hadoop/data/test2/catalina.2017-08-18.log, inode: 1201054, pos: 11
#17/08/28 15:25:50 INFO taildir.TaildirSource: Closed file: /home/hadoop/data/test2/catalina.2017-08-28.log, inode: 1201055, pos: 38

# 查看文件目录:
# $ ls -al test1/*
#-rw-rw-r-- 1 hadoop hadoop 31 Aug 28 15:11 test1/example.log
#-rw-rw-r-- 1 hadoop hadoop  9 Aug 28 15:09 test1/example2.log
# $ ls -al test2/*
#-rw-rw-r-- 1 hadoop hadoop 11 Aug 28 15:08 test2/catalina.2017-08-18.log
#-rw-rw-r-- 1 hadoop hadoop 38 Aug 28 15:13 test2/catalina.2017-08-28.log
#-rw-rw-r-- 1 hadoop hadoop  9 Aug 28 15:12 test2/catalina.out
# 发现只有符合条件的example.log, catalina.2017-08-18.log和catalina.2017-08-28.log被采集到了
# 同时发现发group f1的文件首先被采集, group f2的后被采集, 在group f2中, 修改时间早的catalina.2017-08-18.log文件首先被采集到
# 查看json文件
# $ cat /tmp/flume/taildir_position.json
#[{"inode":1201034,"pos":31,"file":"/home/hadoop/data/test1/example.log"},{"inode":1201054,"pos":11,"file":"/home/hadoop/data/test2/catalina.2017-08-18.log"},{"inode":1201055,"pos":38,"file":"/home/hadoop/data/test2/catalina.2017-08-28.log"}]
# 查看三个文件的字符数:
#[hadoop@BDS-TEST-004 data]$ wc -c test1/example.log
#31 test1/example.log
#[hadoop@BDS-TEST-004 data]$ wc -c test2/catalina.2017-08-18.log
#11 test2/catalina.2017-08-18.log
#[hadoop@BDS-TEST-004 data]$ wc -c test2/catalina.2017-08-28.log
#38 test2/catalina.2017-08-28.log
# 所以json中的offset记录的是文件的字符数, (但是采集数据是按行读取的)
#
##########################################################