# Describe the source
a1.sources=r1
a1.sinks=k1 k2
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=syslogtcp
# 任何主机发送过来的信息都可以采集
a1.sources.r1.host=0.0.0.0
a1.sources.r1.port=22222

a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2
a1.sinkgroups.g1.processor.type = load_balance
a1.sinkgroups.g1.processor.selector=round_robin

# Describe the sink
a1.sinks.k1.type=avro
a1.sinks.k1.hostname=BDS-TEST-001
a1.sinks.k1.port=22223

a1.sinks.k2.type=avro
a1.sinks.k2.hostname=BDS-TEST-002
a1.sinks.k2.port=22223

# Use a channel which buffers events in memory
a1.channels.c1.type=file
a1.channels.c1.checkpointDir=/tmp/flume/example2/checkpoint
a1.channels.c1.dataDirs=/tmp/flume/example2/data

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c1

#################### Flume多级部署实例 #############################
# 选则4台机器, BDS-TEST-001, BDS-TEST-002, BDS-TEST-003, BDS-TEST-004
# 由于hbase是部署在了BDS-TEST-001, BDS-TEST-002上, 所以第一级是BDS-TEST-003, BDS-TEST-004, 第二级是BDS-TEST-001, BDS-TEST-002
# 第一级使用test-example-tier-conf.properties配置, 第二级使用test-example-tier2-conf.properties配置:
#
#                                                 / hadoop
#   avro client ->    BDS-TEST-003    BDS-TEST-001
#                                  \/              \ hbase
#                                  /\              / hbase
#   avro client ->    BDS-TEST-004    BDS-TEST-002
#                                                 \ hadoop
#                       agent a1       agent s2
#
# 启动hbase
# ./hbase shell
# 创建表(末尾不要带分号)
# hbase(main):002:0> create 'flume_example02' , 'cf1'
# 通过list可以看到表创建成功
#
# 注意, 对于多级的部署情况, 需要首先启动后一级的agent, 否则前一级的agent先启动的话, 数据可能发送不出去造成失败等情况发生
# 首先在BDS-TEST-001, BDS-TEST-002启动agent:
# $flume-ng agent -n a2 -f test-example2-tier2-conf.properties
# 然后在BDS-TEST-003, BDS-TEST-004启动agent:
# $flume-ng agent -n a1 -f test-example2-tier1-conf.properties
# 通过管道命令加nc来发送(这里服务我是在003服务器上启动的, 数据是从003上进行发送的):
# cat file01.txt | nc -t BDS-TEST-003 22222
# 看一下file01.txt内容:
#test exec source
#test flume
#test flume 2
#hahahaha
#test file channel
#Test hdfs sink
#file roll sink
#test loadbalance
# 查看hdfs上文件, 先创建/flume/event/example2/170823/Example2-.1503481443421.tmp, 后rename为: /flume/event/example2/170823/Example2-.1503481443421
# 查看hdfs文件内容:
# $ hadoop fs -cat /flume/event/example2/170823/Example2-.1503481443421
#test exec source
#test flume
#test flume 2
#hahahaha
#test file channel
#Test hdfs sink
#file roll sink
#test loadbalance
# 查看hbase中表的内容:
#hbase(main):002:0> scan 'flume_example02'
#ROW                                              COLUMN+CELL
#1503481445440-sjvwDsWKnb-0                      column=cf1:payload, timestamp=1503480937395, value=test exec source
#1503481445448-sjvwDsWKnb-1                      column=cf1:payload, timestamp=1503480937395, value=test flume
#1503481445449-sjvwDsWKnb-2                      column=cf1:payload, timestamp=1503480937395, value=test flume 2
#1503481445449-sjvwDsWKnb-3                      column=cf1:payload, timestamp=1503480937395, value=hahahaha
#1503481445450-sjvwDsWKnb-4                      column=cf1:payload, timestamp=1503480937395, value=test file channel
#1503481445450-sjvwDsWKnb-5                      column=cf1:payload, timestamp=1503480937395, value=Test hdfs sink
#1503481445451-sjvwDsWKnb-6                      column=cf1:payload, timestamp=1503480937395, value=file roll sink
#1503481445452-sjvwDsWKnb-7                      column=cf1:payload, timestamp=1503480937395, value=test loadbalance
#8 row(s) in 0.0910 seconds

# 通过管道命令加nc来发送(这里服务我是在004服务器上启动的, 数据是从004上进行发送的):
# cat file02.txt | nc -t BDS-TEST-004 22222
# 查看file02.txt内容:
#flume
#kafka
#hadoop
#hive
#spark
# 查看hdfs上文件, 发现先创建了/flume/event/example2/170823/Example2-.1503481813818.tmp, 后rename为: /flume/event/example2/170823/Example2-.1503481813818
# 查看hdfs文件内容:
# $ hadoop fs -cat /flume/event/example2/170823/Example2-.1503481813818
#$ hadoop fs -cat /flume/event/example2/170823/Example2-.1503481813818
#flume
#kafka
#hadoop
#hive
#spark
# 查看hbase中表的内容:
#hbase(main):002:0> scan 'flume_example02'
#hbase(main):003:0> scan 'flume_example02'
#ROW                                              COLUMN+CELL
#1503481445440-sjvwDsWKnb-0                      column=cf1:payload, timestamp=1503480937395, value=test exec source
#1503481445448-sjvwDsWKnb-1                      column=cf1:payload, timestamp=1503480937395, value=test flume
#1503481445449-sjvwDsWKnb-2                      column=cf1:payload, timestamp=1503480937395, value=test flume 2
#1503481445449-sjvwDsWKnb-3                      column=cf1:payload, timestamp=1503480937395, value=hahahaha
#1503481445450-sjvwDsWKnb-4                      column=cf1:payload, timestamp=1503480937395, value=test file channel
#1503481445450-sjvwDsWKnb-5                      column=cf1:payload, timestamp=1503480937395, value=Test hdfs sink
#1503481445451-sjvwDsWKnb-6                      column=cf1:payload, timestamp=1503480937395, value=file roll sink
#1503481445452-sjvwDsWKnb-7                      column=cf1:payload, timestamp=1503480937395, value=test loadbalance
#1503481810655-sjvwDsWKnb-8                      column=cf1:payload, timestamp=1503481302434, value=flume
#1503481810655-sjvwDsWKnb-9                      column=cf1:payload, timestamp=1503481302434, value=kafka
#1503481810656-sjvwDsWKnb-10                     column=cf1:payload, timestamp=1503481302434, value=hadoop
#1503481810656-sjvwDsWKnb-11                     column=cf1:payload, timestamp=1503481302434, value=hive
#1503481810657-sjvwDsWKnb-12                     column=cf1:payload, timestamp=1503481302434, value=spark
# 发现新的数据也插入进来了
##########################################################