# Describe the source
a1.sources=r1
a1.sinks=k1 k2
a1.channels=c1 c2

# Describe/configure the source
a1.sources.r1.type=http
#对所有的主机开放
a1.sources.r1.bind=0.0.0.0
a1.sources.r1.port=7777
a1.sources.r1.selector.type=multiplexing
a1.sources.r1.selector.header=region
a1.sources.r1.selector.mapping.east=c1
a1.sources.r1.selector.mapping.middle=c2
a1.sources.r1.selector.default=c2

# Describe the sink
a1.sinks.k1.type=hdfs
# /flume/event/目录需要先创建, hadoop fs -mkdir -p /flume/event/
a1.sinks.k1.hdfs.path=/flume/event/%y%m%d
a1.sinks.k1.hdfs.filePrefix=channel-multiplexing
#是否轮转生成新的hdfs文件
a1.sinks.k1.hdfs.round=true
#每一分钟轮转一次, 生成新的文件, 但是没有接收到event的话是不会每分钟都生成新的文件的
a1.sinks.k1.hdfs.roundValue=1
a1.sinks.k1.hdfs.roundUnit=minute
# 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp=true
a1.sinks.k1.hdfs.writeFormat=Text

# Describe the sink
a1.sinks.k2.type=hbase
a1.sinks.k2.table=flume_multiplexing_test
a1.sinks.k2.columnFamily=cf1
a1.sinks.k2.serializer=org.apache.flume.sink.hbase.RegexHbaseEventSerializer

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

a1.channels.c2.type=memory
a1.channels.c2.capacity=1000
a1.channels.c2.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1 c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2

#################################################
# 在Hbase中创建表flume_multiplexing_test, sh hbase shell
# create 'flume_multiplexing_test', 'cf1'
# 启动agent代理:
# $flume-ng agent -n a1 -f channel-multiplexing-conf.properties
# 使用curl发送http请求
# curl -X POST -d '[{"headers" :{"region":"east", "type":"dev"}, "body": "test east data01"}]' http://BDS-TEST-001:7777
# curl -X POST -d '[{"headers" :{"region":"middle", "type":"dev"}, "body": "test middle data01"}]' http://BDS-TEST-001:7777
# curl -X POST -d '[{"headers" :{"region":"north"}, "body": "test north data01"}]' http://BDS-TEST-001:7777
# 在hdfs上, 先创建/flume/event/170821/channel-multiplexing.1503320137907.tmp, 然后重命名为/flume/event/170821/channel-multiplexing.1503320137907
# 查看文件内容:
# $ hadoop fs -text /flume/event/170821/channel-multiplexing.1503320137907
#1503320138128   test east data01
# 查看hbase中的数据:
# scan 'flume_multiplexing_test'
#ROW                                              COLUMN+CELL
#1503320259732-a5B8VBfwUz-0                      column=cf1:payload, timestamp=1503320262907, value=test middle data01
#1503320431914-a5B8VBfwUz-1                      column=cf1:payload, timestamp=1503320434917, value=test north data01
#2 row(s) in 0.0110 seconds
# 可以发现, 第一个post的请求数据发送到了hdfs中, 第二和第三个请求发送到了hbase中
##########################################################