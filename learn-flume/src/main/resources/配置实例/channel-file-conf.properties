# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log

# Describe the sink(logger是直接打印到控制台)
a1.sinks.k1.type=logger

# Use a channel which buffers events in memory
a1.channels.c1.type=file
a1.channels.c1.checkpointDir=/tmp/flume/checkpoint
a1.channels.c1.dataDirs=/tmp/flume/data

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 首先要创建目录/tmp/flume, 子目录可以不创建, 实际测试发现, 父目录完全不用创建
# 启动agent代理:
# $flume-ng agent -n a1 -f channel-file-conf.properties
# 在另一个session中准备源数据
# $echo "test file channel" > file-exec.log
# 这时另一个窗口中已经可以看到数据了
# 继续追加数据(是>>, 如果是>是overwrite是接收不到的)
# echo "test file channel again" >> file-exec.log
##########################################################