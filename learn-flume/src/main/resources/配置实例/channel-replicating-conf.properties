# Describe the source
a1.sources=r1
a1.sinks=k1 k2
a1.channels=c1 c2

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log
a1.sources.r1.selector.type=replicating

# Describe the sink
a1.sinks.k1.type=hdfs
# 目录不用自己创建, 会自动创建
a1.sinks.k1.hdfs.path=/flume/event/%y%m%d
a1.sinks.k1.hdfs.filePrefix=channel-replicating
#是否轮转生成新的hdfs文件
a1.sinks.k1.hdfs.round=true
#每一分钟轮转一次, 生成新的文件, 但是没有接收到event的话是不会每分钟都生成新的文件的
a1.sinks.k1.hdfs.roundValue=1
a1.sinks.k1.hdfs.roundUnit=minute
# 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp=true
a1.sinks.k1.hdfs.writeFormat=Text

# Describe the sink
a1.sinks.k2.type=hbase
a1.sinks.k2.table=flume_replicating_test
a1.sinks.k2.columnFamily=cf1
a1.sinks.k2.serializer=org.apache.flume.sink.hbase.RegexHbaseEventSerializer

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

a1.channels.c2.type=memory
a1.channels.c2.capacity=1000
a1.channels.c2.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1 c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2

#################################################
# 在Hbase中创建表flume_replicating_test, sh hbase shell
# create 'flume_replicating_test', 'cf1'
# 启动agent代理:
# $flume-ng agent -n a1 -f channel-replicating-conf.properties
# 对于hdfs, 先在目录里面写.tmp临时文件, 写后改成不带.tmp的文件, 比如: /flume/event/170821/channel-replicating.1503299744998.tmp, 最终变成: /flume/event/170821/channel-replicating.1503299744998
# $ hadoop fs -text /flume/event/170821/channel-replicating.1503299744998
#1503299745215   test exec source
#1503299745217   test flume
#1503299745219   test flume 2
#1503299745220   hahahaha
#1503299745221   test file channel
#1503299745222   Test hdfs sink
#1503299745223   file roll sink
#1503299745223   test loadbalance
# 查看hbase中的数据:
#hbase(main):011:0> scan 'flume_replicating_test'
#ROW                                              COLUMN+CELL
#1503299744087-uaO7ow0VIh-0                      column=cf1:payload, timestamp=1503299747284, value=test exec source
#1503299744093-uaO7ow0VIh-1                      column=cf1:payload, timestamp=1503299747284, value=test flume
#1503299744093-uaO7ow0VIh-2                      column=cf1:payload, timestamp=1503299747284, value=test flume 2
#1503299744094-uaO7ow0VIh-3                      column=cf1:payload, timestamp=1503299747284, value=hahahaha
#1503299744094-uaO7ow0VIh-4                      column=cf1:payload, timestamp=1503299747284, value=test file channel
#1503299744094-uaO7ow0VIh-5                      column=cf1:payload, timestamp=1503299747284, value=Test hdfs sink
#1503299744095-uaO7ow0VIh-6                      column=cf1:payload, timestamp=1503299747284, value=file roll sink
#1503299744095-uaO7ow0VIh-7                      column=cf1:payload, timestamp=1503299747284, value=test loadbalance
#8 row(s) in 0.0160 seconds
# 我们发现通过replicating, 不同sink中的数据是一样的
##########################################################