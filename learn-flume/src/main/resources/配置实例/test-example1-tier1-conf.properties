# Describe the source
a1.sources=r1
a1.sinks=k1 k2
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=avro
# 任何主机发送过来的信息都可以采集
a1.sources.r1.bind=0.0.0.0
a1.sources.r1.port=33333

a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2
a1.sinkgroups.g1.processor.type = load_balance
a1.sinkgroups.g1.processor.selector=round_robin

# Describe the sink
a1.sinks.k1.type=avro
a1.sinks.k1.hostname=BDS-TEST-003
a1.sinks.k1.port=33334

a1.sinks.k2.type=avro
a1.sinks.k2.hostname=BDS-TEST-004
a1.sinks.k2.port=33334

# Use a channel which buffers events in memory
a1.channels.c1.type=file
a1.channels.c1.checkpointDir=/tmp/flume/example1/checkpoint
a1.channels.c1.dataDirs=/tmp/flume/example1/data

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c1

#################### Flume多级部署实例 #############################
# 选则4台机器, BDS-TEST-001, BDS-TEST-002, BDS-TEST-003, BDS-TEST-004
# 第一级是BDS-TEST-001, BDS-TEST-002, 第二级是BDS-TEST-003, BDS-TEST-004
# 第一级使用test-example-tier-conf.properties配置, 第二级使用test-example-tier2-conf.properties配置:
#
#
#   avro client ->    BDS-TEST-001    BDS-TEST-003
#                                  \/
#                                  /\
#   avro client ->    BDS-TEST-002    BDS-TEST-004
#
#                       agent a1       agent s2
#
# 注意, 对于多级的部署情况, 需要首先启动后一级的agent, 否则前一级的agent先启动的话, 数据可能发送不出去造成失败等情况发生
# 首先在BDS-TEST-003, BDS-TEST-004启动agent:
# $flume-ng agent -n a2 -f test-example1-tier2-conf.properties
# 然后在BDS-TEST-001, BDS-TEST-002启动agent:
# $flume-ng agent -n a1 -f test-example1-tier1-conf.properties
# 然后在BDS-TEST-001(可以是任意机器, 只要与BDS-TEST-001网络相同即可)通过avro client向BDS-TEST-001发送数据:
# $flume-ng avro-client -H localhost -p 33333 -F file-exec.log
# $ cat file-exec.log
#test exec source
#test flume
#test flume 2
#hahahaha
#test file channel
#Test hdfs sink
#file roll sink
#test loadbalance
# 然后去hadoop上进行查看, 先创建了文件:/flume/event/example1/170823/Example-.1503476394249.tmp, 之后rename为:/flume/event/example1/170823/Example-.1503476394249
# 查看文件内容:
#$ hadoop fs -text /flume/event/example1/170823/Example-.1503476394249
#test exec source
#test flume
#test flume 2
#hahahaha
#test file channel
#Test hdfs sink
#file roll sink
#test loadbalance
# 然后在BDS-TEST-002(可以是任意机器, 只要与BDS-TEST-001网络相同即可)通过avro client向BDS-TEST-002发送数据:
# $flume-ng avro-client -H localhost -p 33333 -F file-exec2.log
# $ cat file-exec2.log
#flume
#kafka
#hadoop
#hive
#spark
# 然后去hadoop上进行查看, 先创建了文件:/flume/event/example1/170823/Example-.1503476590828.tmp, 之后rename为:/flume/event/example1/170823/Example-.1503476590828
# 查看hdfs文件内容:
# $ hadoop fs -text /flume/event/example1/170823/Example-.1503476590828
#flume
#kafka
#hadoop
#hive
#spark
# 多发几次数据可以发现, 数据是轮询的发送给BDS-TEST-003, BDS-TEST-004
#
# 注意: 停止的时候先停止第一层的agent, 然后停止第二层的agent, 防止第二层的agent停止后第一层的agent又有数据进行发送
##########################################################