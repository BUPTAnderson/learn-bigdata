# Describe the source
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# Describe/configure the source
a1.sources.r1.type=exec
a1.sources.r1.command = tail -F /home/hadoop/data/file-exec.log

# Describe the sink(logger是直接打印到控制台)
a1.sinks.k1.type=hdfs
# /flume/event/目录需要先创建, hadoop fs -mkdir -p /flume/event/
a1.sinks.k1.hdfs.path=/flume/event/%y%m%d
a1.sinks.k1.hdfs.filePrefix=Test-
#是否轮转生成新的hdfs文件
a1.sinks.k1.hdfs.round=true
#每一分钟轮转一次, 生成新的文件, 但是没有接收到event的话是不会每分钟都生成新的文件的
a1.sinks.k1.hdfs.roundValue=1
a1.sinks.k1.hdfs.roundUnit=minute
# 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp=true

# Use a channel which buffers events in memory
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

# Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

#################################################
# 启动agent代理:
# $flume-ng agent -n a1 -f sink-hdfs-conf.properties
# 会先在目录里面写.tmp临时文件, 写好后改成不带.tmp的文件, 比如: /flume/event/170820/Test-.1503221613136.tmp, 最终变成: /flume/event/170820/Test-.1503221613136
# 使用hadoop fs -text /flume/event/170820/Test-.1503221613136查看文件内容:
# 1503221613202   54 65 73 74 20 68 64 66 73 20 73 69 6e 6b
# 原始内容是:Test hdfs sink
##########################################################